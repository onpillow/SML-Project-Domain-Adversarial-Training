{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "g0_NnIyZqNEh"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
        "from torch.utils.data import DataLoader, Dataset, random_split"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Loading and Preprocessing\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "8JVzQN2zrAJl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "# Load data function\n",
        "def load_data(file_path):\n",
        "    with open(file_path, 'r') as f:\n",
        "        return [json.loads(line) for line in f]\n",
        "\n",
        "domain1_data = load_data('domain1_train.json')\n",
        "domain2_data = load_data('domain2_train.json')\n",
        "domain2_data = [entry for entry in domain2_data if len(entry['text']) > 0]\n"
      ],
      "metadata": {
        "id": "qdpB1CTxqOrr"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data\n",
        "train_data_domain, valid_data_domain = train_test_split(domain1_data+domain2_data, test_size=0.2, random_state=42)\n",
        "\n",
        "train_data = train_data_domain\n",
        "valid_data = valid_data_domain"
      ],
      "metadata": {
        "id": "Xc-i0rewaXUe"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create PyTorch Datasets and DataLoaders"
      ],
      "metadata": {
        "id": "Zgs2cz5BrIf6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TextDataset(Dataset):\n",
        "    def __init__(self, data):\n",
        "        self.data = data\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = torch.tensor(self.data[idx]['text'])\n",
        "        label = torch.tensor(self.data[idx]['label'])\n",
        "        return text, label\n",
        "\n",
        "\n",
        "\n",
        "def collate_batch(batch):\n",
        "    texts, labels = zip(*batch)\n",
        "    text_lengths = [len(txt) for txt in texts]\n",
        "    texts = pad_sequence(texts, batch_first=True)\n",
        "    labels = torch.tensor(labels).float()  # Convert labels to float\n",
        "    return texts, labels, text_lengths\n",
        "\n",
        "\n",
        "# DataLoader with the custom collate function\n",
        "train_dataset = TextDataset(train_data)\n",
        "valid_dataset = TextDataset(valid_data)\n",
        "\n",
        "batch_size = 64\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_batch)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_batch)"
      ],
      "metadata": {
        "id": "XIStSVSprCRn"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Definition"
      ],
      "metadata": {
        "id": "S3ybfyRcrMhp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BiLSTMClassifier(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, bidirectional=True, dropout=0.5):\n",
        "        super(BiLSTMClassifier, self).__init__()\n",
        "\n",
        "        # Text embedding layer\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "        # BiLSTM layer\n",
        "        self.rnn = nn.LSTM(embedding_dim, hidden_dim, num_layers=n_layers, bidirectional=bidirectional, dropout=dropout, batch_first=True)\n",
        "\n",
        "        # Classifier layer\n",
        "        self.fc = nn.Linear(hidden_dim*2, output_dim)  # x2 for bidirectional\n",
        "\n",
        "        # Dropout layer\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, text, text_lengths):\n",
        "        embedded = self.dropout(self.embedding(text))\n",
        "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths, batch_first=True, enforce_sorted=False)\n",
        "        packed_output, (hidden, cell) = self.rnn(packed_embedded)\n",
        "        output, _ = nn.utils.rnn.pad_packed_sequence(packed_output, batch_first=True)\n",
        "\n",
        "        # Feature from the last hidden state of the BiLSTM\n",
        "        hidden = self.dropout(torch.cat((hidden[-2, :, :], hidden[-1, :, :]), dim=1))\n",
        "\n",
        "        # Return the classifier's output\n",
        "        return self.fc(hidden).squeeze(1)"
      ],
      "metadata": {
        "id": "X_5QNJC6aX1w"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Find class weight"
      ],
      "metadata": {
        "id": "-gz-HSterSkt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "# Extract model counts for domain 2\n",
        "models_domain2 = [entry['label'] for entry in domain2_data]\n",
        "class_counts_domain2 = [models_domain2.count(float(i)) for i in range(int(max(models_domain2)) + 1)]\n",
        "\n",
        "# Compute class weights for domain 2\n",
        "total_samples_domain2 = sum(class_counts_domain2)\n",
        "class_weights_domain2 = torch.tensor([total_samples_domain2 / count for count in class_counts_domain2]).float().to(device)\n",
        "\n",
        "def get_batch_weights(labels, class_weights_domain2):\n",
        "    return class_weights_domain2[labels.long()].unsqueeze(1)\n",
        "\n"
      ],
      "metadata": {
        "id": "1iJ7oVyOWQyv"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_counts_domain2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mmvmLTymemJH",
        "outputId": "c34829a8-58ba-47e7-c992-d41d535d2dba"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[12750, 2149]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loss Function and Optimizer"
      ],
      "metadata": {
        "id": "1pfzTu_zd7vA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Instantiate the model\n",
        "model = BiLSTMClassifier(vocab_size=5000, embedding_dim=128, hidden_dim=256, output_dim=1, n_layers=2).to(device)\n",
        "\n",
        "# Classification criterion\n",
        "classification_criterion = nn.BCEWithLogitsLoss(pos_weight=class_weights_domain2) ##change\n",
        "\n",
        "\n",
        "# Optimizer\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "# Binary accuracy function\n",
        "def binary_accuracy(predictions, y):\n",
        "    rounded_preds = torch.round(torch.sigmoid(predictions)).squeeze()  # Ensure it's a 1D tensor\n",
        "    correct = (rounded_preds == y).float()\n",
        "    return correct.sum() / len(correct)\n",
        "\n",
        "\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "def compute_f1(predictions, labels):\n",
        "    # Convert predictions to binary\n",
        "    preds_binary = torch.round(torch.sigmoid(predictions))\n",
        "    preds_binary = preds_binary.detach().cpu().numpy()\n",
        "    labels = labels.detach().cpu().numpy()\n",
        "\n",
        "    return f1_score(labels, preds_binary)\n",
        "\n"
      ],
      "metadata": {
        "id": "EIcgKM7lrRAZ"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training Loop"
      ],
      "metadata": {
        "id": "4reXqMDGrZmZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "num_epochs = 10\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    epoch_f1 = 0  # Initialize the F1-score for the epoch\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    # Stage 1: Train main classifier\n",
        "    for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1} Classifier Training\"):\n",
        "        texts, labels, text_lengths = batch\n",
        "        texts = texts.to(device)\n",
        "        labels = labels.to(device)\n",
        "        #text_lengths = torch.tensor(text_lengths).long()\n",
        "\n",
        "        batch_weights = get_batch_weights(labels, class_weights_domain2).to(device) # Get batch-specific weights\n",
        "\n",
        "        # Create a criterion for the current batch with the specified pos_weight\n",
        "        batch_criterion = nn.BCEWithLogitsLoss(pos_weight=batch_weights.squeeze()).to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Get the classifier's predictions\n",
        "        predictions = model(texts, text_lengths)\n",
        "        loss = batch_criterion(predictions, labels)\n",
        "        acc = binary_accuracy(predictions, labels)\n",
        "        f1 = compute_f1(predictions, labels)  # Compute F1 score\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "        epoch_acc += acc.item()\n",
        "        epoch_f1 += f1\n",
        "\n",
        "\n",
        "    print(f\"Epoch {epoch+1} Classifier Training: Loss: {epoch_loss/len(train_loader):.3f} | Accuracy: {epoch_acc/len(train_loader):.3f} | F1-Score: {epoch_f1/len(train_loader):.3f}\")\n",
        "\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    epoch_f1 = 0  # Reset the F1-score for the next epoch\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LgzufMuRrbTo",
        "outputId": "f896a9d2-5201-403c-b985-2caafde6d841"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1 Classifier Training: 100%|██████████| 430/430 [00:46<00:00,  9.22it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 Classifier Training: Loss: 1.043 | Accuracy: 0.625 | F1-Score: 0.634\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2 Classifier Training: 100%|██████████| 430/430 [00:44<00:00,  9.73it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2 Classifier Training: Loss: 0.878 | Accuracy: 0.670 | F1-Score: 0.662\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3 Classifier Training: 100%|██████████| 430/430 [00:45<00:00,  9.35it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3 Classifier Training: Loss: 0.789 | Accuracy: 0.717 | F1-Score: 0.694\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4 Classifier Training: 100%|██████████| 430/430 [00:46<00:00,  9.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4 Classifier Training: Loss: 0.745 | Accuracy: 0.754 | F1-Score: 0.724\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5 Classifier Training: 100%|██████████| 430/430 [00:43<00:00,  9.93it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5 Classifier Training: Loss: 0.713 | Accuracy: 0.764 | F1-Score: 0.735\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6 Classifier Training: 100%|██████████| 430/430 [00:44<00:00,  9.75it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6 Classifier Training: Loss: 0.715 | Accuracy: 0.762 | F1-Score: 0.734\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7 Classifier Training: 100%|██████████| 430/430 [00:42<00:00, 10.00it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7 Classifier Training: Loss: 0.683 | Accuracy: 0.771 | F1-Score: 0.740\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8 Classifier Training: 100%|██████████| 430/430 [00:45<00:00,  9.52it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8 Classifier Training: Loss: 0.640 | Accuracy: 0.792 | F1-Score: 0.760\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9 Classifier Training: 100%|██████████| 430/430 [00:44<00:00,  9.66it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9 Classifier Training: Loss: 0.624 | Accuracy: 0.810 | F1-Score: 0.774\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10 Classifier Training: 100%|██████████| 430/430 [00:43<00:00,  9.91it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10 Classifier Training: Loss: 0.594 | Accuracy: 0.816 | F1-Score: 0.781\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 11 Classifier Training: 100%|██████████| 430/430 [00:45<00:00,  9.48it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 11 Classifier Training: Loss: 0.575 | Accuracy: 0.825 | F1-Score: 0.790\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 12 Classifier Training: 100%|██████████| 430/430 [00:44<00:00,  9.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 12 Classifier Training: Loss: 0.555 | Accuracy: 0.827 | F1-Score: 0.792\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 13 Classifier Training: 100%|██████████| 430/430 [00:44<00:00,  9.59it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 13 Classifier Training: Loss: 0.536 | Accuracy: 0.828 | F1-Score: 0.792\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 14 Classifier Training: 100%|██████████| 430/430 [00:43<00:00,  9.83it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 14 Classifier Training: Loss: 0.502 | Accuracy: 0.839 | F1-Score: 0.806\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 15 Classifier Training: 100%|██████████| 430/430 [00:45<00:00,  9.50it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 15 Classifier Training: Loss: 0.499 | Accuracy: 0.848 | F1-Score: 0.813\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation"
      ],
      "metadata": {
        "id": "3UOIp22O1IML"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, valid_loader, device):\n",
        "    model.eval()\n",
        "    all_predictions = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():  # Disable gradient computation for efficiency\n",
        "        for batch in valid_loader:\n",
        "            texts, labels, text_lengths = batch\n",
        "            texts = texts.to(device)\n",
        "            labels = labels.float().to(device)\n",
        "\n",
        "            # Compute model predictions\n",
        "            predictions = model(texts, text_lengths)\n",
        "            if predictions.dim() > 1 and predictions.size(1) == 1:\n",
        "                predictions = predictions.squeeze(1)\n",
        "            all_predictions.extend(predictions.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    binary_predictions = [1 if p >= 0.5 else 0 for p in all_predictions]\n",
        "\n",
        "    # Convert lists to tensors\n",
        "    tensor_predictions = torch.tensor(binary_predictions)\n",
        "    tensor_labels = torch.tensor(all_labels)\n",
        "\n",
        "    accuracy = binary_accuracy(tensor_predictions, tensor_labels)\n",
        "\n",
        "    # Compute F1-Score\n",
        "    f1 = f1_score(all_labels, binary_predictions, average='macro')\n",
        "\n",
        "    return accuracy, f1\n",
        "\n",
        "# After the training loop, evaluate on validation set\n",
        "valid_accuracy, valid_f1 = evaluate_model(model, valid_loader, device)\n",
        "print(f\"Validation Accuracy: {valid_accuracy:.4f}\")\n",
        "print(f\"Validation F1-Score: {valid_f1:.4f}\")\n"
      ],
      "metadata": {
        "id": "yg3GT09kuGwE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "89bd30f4-89a7-4ca2-9bef-0a59997612b3"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.8392\n",
            "Validation F1-Score: 0.8328\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Send to Kaggle"
      ],
      "metadata": {
        "id": "u21DC-CtCk1O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import json\n",
        "\n",
        "# Load the test data\n",
        "with open('test_set.json', 'r') as f:\n",
        "    test_data = [json.loads(line) for line in f]\n",
        "# Evaluate on test data\n",
        "model.eval()\n",
        "test_results = []"
      ],
      "metadata": {
        "id": "9yHC0Q5RCkFr"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "    for entry in tqdm(test_data, desc=\"Evaluating Test Data\"): # change from test_set to test_data\n",
        "        text = entry[\"text\"]\n",
        "        text_tensor = torch.tensor(text).unsqueeze(0).to(device)  # Adding an extra batch dimension\n",
        "        text_length = torch.tensor([len(text)])  # Sequence length for current entry\n",
        "\n",
        "        # Pass the sequence and its length to the model\n",
        "        prediction = model(text_tensor, text_length)\n",
        "        if prediction.dim() > 1 and prediction.size(1) == 1:\n",
        "            prediction = prediction.squeeze(1)\n",
        "        prediction = torch.sigmoid(prediction).item()  # Convert raw score to value between 0 and 1\n",
        "\n",
        "        # Classify the texts\n",
        "        class_label = 1 if prediction >= 0.5 else 0\n",
        "\n",
        "        test_results.append({\n",
        "            \"id\": entry[\"id\"],\n",
        "            \"class\": class_label\n",
        "        })"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U3SCGp_zCkLt",
        "outputId": "eff96068-1340-463b-c14c-afd1d369a231"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating Test Data: 100%|██████████| 1000/1000 [00:05<00:00, 186.33it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Write results to CSV\n",
        "with open('results.csv', 'w', newline='') as csvfile:\n",
        "    fieldnames = ['id', 'class']\n",
        "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
        "\n",
        "    writer.writeheader()\n",
        "    for result in test_results:\n",
        "        writer.writerow(result)"
      ],
      "metadata": {
        "id": "daA24OYRCkRM"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rLOUCmxhHNR6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tCOxiyFfHNTt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TQonG8LeHNVn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}